{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "581230c4-9cf8-4aa5-bd23-104c536dcda6",
   "metadata": {},
   "source": [
    "# Load and Clean the data\n",
    "* Filter out rows with missing data in the required columns (REQUIRED_COLS)\n",
    "* Keep only entries that are not marked as retracted\n",
    "* Remove duplicates\n",
    "* Save cleaned data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03211bef-65de-4038-b826-fb4671cae6eb",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "Data was [downloaded from OpenAlex](https://openalex.org/works?page=1&filter=default.search%3ATopic%20Network%20Graph,default.search%3ABibliometrics,default.search%3A%22Machine%20Learning%22%20OR%20%22Large%20Language%20Model%22,type%3Atypes%2Farticle,publication_year%3A2019-2024,language%3Alanguages%2Fen&group_by=type,authorships.institutions.lineage,primary_topic.id,open_access.is_oa,publication_year,authorships.author.id,keywords.id,authorships.countries,institutions.is_global_south,best_oa_location.license,primary_location.source.publisher_lineage,is_retracted,authorships.institutions.continent,corresponding_author_ids,primary_topic.domain.id,primary_topic.field.id,grants.funder,open_access.any_repository_has_fulltext,primary_location.source.is_oa,has_doi,best_oa_location.is_accepted,corresponding_institution_ids,has_orcid,authorships.institutions.type,primary_location.source.id,best_oa_location.is_published,language,repository,primary_location.source.is_in_doaj)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd8ff49b-fd74-428a-b81d-3a14807c877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile as zf\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e95c830-f4ee-4547-a822-93a9bae66357",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data'  # path to folder with (downloaded or generated) data, will not be committed to git\n",
    "ACRONYM = 'openalex-tng'  # Acronym, aka machine readable name, of this dataset ('tng' stands for \"topic netework graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a0c3c99-18fa-46d0-973a-0c5e95d0c272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The downloaded data set has 1349 rows and 173 columns.\n"
     ]
    }
   ],
   "source": [
    "data_df = pd.read_csv(os.path.join(DATA_PATH, f\"{ACRONYM}.csv\"), index_col=0)\n",
    "print(f\"The downloaded data set has {data_df.shape[0]} rows and {data_df.shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6bca1a-0dfc-442f-b4e6-be9bc6a6b8dc",
   "metadata": {},
   "source": [
    "## Clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c310624-d7e1-49ee-9a2e-2497a765013a",
   "metadata": {},
   "source": [
    "### Filter out rows with missing data in the required columns (REQUIRED_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30f16acd-78e7-40a3-8bed-3905b7b514d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns used in the analysis\n",
    "USED_COLS = [\n",
    "    'authorships.author.display_name', 'publication_year', 'title', 'primary_location.source.display_name','doi', \n",
    "    'has_fulltext', 'is_retracted',\n",
    "    'abstract', 'open_access.is_oa', 'authorships.countries']\n",
    "# columns required to have a value\n",
    "REQUIRED_COLS = [val for val in USED_COLS if val not in ['doi', 'open_access.is_oa', 'authorships.countries']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ce97099-b013-4520-9d3e-99889af419c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 5 entries without authorships.author.display_name.\n",
      "Detected 0 entries without publication_year.\n",
      "Detected 0 entries without title.\n",
      "Detected 42 entries without primary_location.source.display_name.\n",
      "Detected 0 entries without has_fulltext.\n",
      "Detected 0 entries without is_retracted.\n",
      "Detected 55 entries without abstract.\n"
     ]
    }
   ],
   "source": [
    "# First count rows with missing data in the required columns (REQUIRED_COLS)\n",
    "clean_df = data_df\n",
    "for col in REQUIRED_COLS:\n",
    "    idx = clean_df[col].isna()\n",
    "    print(f\"Detected {idx.sum()} entries without {col}.\")\n",
    "\n",
    "# Filter out rows with missing data in the required columns (REQUIRED_COLS)\n",
    "clean_df = data_df\n",
    "for col in REQUIRED_COLS:\n",
    "    idx = clean_df[col].isna()\n",
    "    clean_df = clean_df[~idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0565996-212d-4b20-a129-a43ac6638f3e",
   "metadata": {},
   "source": [
    "### Remove retracted articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82b5a5e8-cb11-41f4-87d8-df70f35b1e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of articles that have been retacted: 2\n"
     ]
    }
   ],
   "source": [
    "# remove retracted articles\n",
    "idx = clean_df['is_retracted']\n",
    "clean_df = clean_df[~idx]\n",
    "print(f\"Count of articles that have been retacted: {idx.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47270a8c-ccc9-4faf-be60-958b59c4b9d0",
   "metadata": {},
   "source": [
    "### Keep only articles with full text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daf7f0a9-5e7d-4449-908c-52557138e8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of articles that do not have full text: 1\n"
     ]
    }
   ],
   "source": [
    "# remove articles without fulltext\n",
    "idx = clean_df['has_fulltext']\n",
    "clean_df = clean_df[idx]\n",
    "print(f\"Count of articles that do not have full text: {(~idx).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c331ee-7aac-434f-9681-8acabe3a81d7",
   "metadata": {},
   "source": [
    "### Drop duplicates\n",
    "Duplicates are identified based on the values in the used columns (USED_COLS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83cfc012-bec1-46cd-bf86-9bde3c96fe24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 duplicated rows\n"
     ]
    }
   ],
   "source": [
    "# remove duplicated rows\n",
    "print(f\"Found {clean_df.duplicated(subset=USED_COLS).sum()} duplicated rows\")\n",
    "clean_df.drop_duplicates(subset=USED_COLS, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6b7b8f6-f2d6-4124-97c0-c70505bdd0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleanup, the dataset has 1249 rows and 173 columns\n"
     ]
    }
   ],
   "source": [
    "print(f\"After cleanup, the dataset has {clean_df.shape[0]} rows and {clean_df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a604c7dd-eb82-42ae-9e9f-6d40274b24f6",
   "metadata": {},
   "source": [
    "## Save cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06018ab5-da3d-45e7-a54f-98284572499c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(filename, path, df):\n",
    "    \"\"\"Writes a zipped file with a correctly named csv file inside.\"\"\"\n",
    "    with zf.ZipFile(path, 'w') as ziparchive:\n",
    "        ziparchive.writestr(filename, df.to_csv())\n",
    "\n",
    "csvname = f\"{ACRONYM}-clean.csv\"\n",
    "path = os.path.join(DATA_PATH, f\"{csvname}.zip\")\n",
    "save_dataset(csvname, path, clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68740654-b069-4add-8824-cbdbf5cfc025",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
